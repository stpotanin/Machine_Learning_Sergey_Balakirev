{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt9LgzgYbVzJPUpue1kcIT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1370101/step/5"
      ],
      "metadata": {
        "id": "_ijVl46jV-sG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Линейный дискриминант Фишера можно записать в виде:\n",
        "$$\n",
        "a(x) = argmax_{y∈Y}(x^T⋅α_y + β_y)\n",
        "$$\n",
        "\n",
        "где\n",
        "\n",
        "$$\n",
        "α_y = \\hat Σ^{-1}⋅\\hatμ_y\n",
        "$$\n",
        "\n",
        "$$\n",
        "β_y = ln(λy⋅Py) - 0,5 ⋅ \\hat μ_y^T ⋅ \\hat Σ^ {-1} ⋅ \\hat μ^y\n",
        "$$"
      ],
      "metadata": {
        "id": "8Lhtt7bujZSR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCl6arNKV7gJ",
        "outputId": "d1a6fbb1-4dfb-4ed6-ed41-6dbdeb8f6056"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2.36511519, -3.65174678,  1.40519463]),\n",
              " array([-1.09197193,  3.10341471, -1.09191239]))"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# исходные параметры распределений двух классов\n",
        "np.random.seed(0)\n",
        "mean1 = np.array([1, -2, 0])\n",
        "mean2 = np.array([1, 3, 1])\n",
        "r = 0.7\n",
        "D = 2.0\n",
        "V = [[D, D * r, D*r*r], [D*r, D, D*r], [D*r*r, D*r, D]] # три предиктора\n",
        "\n",
        "# моделирование обучающей выборки\n",
        "N = 1000\n",
        "x1 = np.random.multivariate_normal(mean1, V, N).T\n",
        "x2 = np.random.multivariate_normal(mean2, V, N).T\n",
        "\n",
        "x_train = np.hstack([x1, x2]).T\n",
        "y_train = np.hstack([np.zeros(N), np.ones(N)])\n",
        "\n",
        "# вычисление оценок МО и общей ковариационной матрицы\n",
        "mm1 = np.mean(x1.T, axis=0)\n",
        "mm2 = np.mean(x2.T, axis=0)\n",
        "\n",
        "a = np.hstack([(x1.T - mm1).T, (x2.T - mm2).T])\n",
        "VV = np.array([\n",
        "    [np.dot(a[0], a[0]) / (N*2-1), np.dot(a[0], a[1]) / (N*2-1), np.dot(a[0], a[2]) / (N*2-1)],\n",
        "    [np.dot(a[1], a[0]) / (N*2-1), np.dot(a[1], a[1]) / (N*2-1), np.dot(a[1], a[2]) / (N*2-1)],\n",
        "    [np.dot(a[2], a[0]) / (N*2-1), np.dot(a[2], a[1]) / (N*2-1), np.dot(a[2], a[2]) / (N*2-1)]\n",
        "])\n",
        "\n",
        "# параметры для линейного дискриминанта Фишера\n",
        "Py1, L1 = 0.5, 1  # вероятности появления классов\n",
        "Py2, L2 = 1 - Py1, 1  # и величины штрафов неверной классификации\n",
        "\n",
        "# Функции:\n",
        "alpha = lambda x, v, m: m @ np.linalg.inv(v)\n",
        "beta = lambda x, v, m, l, py: np.log(l * py) - 0.5 * m @ np.linalg.inv(v) @ m\n",
        "am = lambda x: np.argmax([alpha(x, VV, mm1) @ x.T + beta(x, VV, mm1, L1, Py1),\n",
        "                          alpha(x, VV, mm2) @ x.T + beta(x, VV, mm2, L2, Py2)]) # классификатор\n",
        "\n",
        "# Ответ на вопрос задания\n",
        "alpha1, alpha2  = alpha(x_train, VV, mm1), alpha(x_train, VV, mm2)\n",
        "beta1, beta2 = beta(x_train, VV, mm1, L1, Py1), beta(x_train, VV, mm2, L2, Py2)\n",
        "\n",
        "# Предсказание по модели\n",
        "predict = [am(x) for x in x_train]\n",
        "\n",
        "# Качество\n",
        "Q = sum(predict != y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Вычисление матрицы ковариации вручную\n",
        "# исходные параметры распределений двух классов\n",
        "np.random.seed(0)\n",
        "mean1 = np.array([1, -2, 0])\n",
        "mean2 = np.array([1, 3, 1])\n",
        "r = 0.7\n",
        "D = 2.0\n",
        "V = [[D, D * r, D*r*r], [D*r, D, D*r], [D*r*r, D*r, D]] # три предиктора\n",
        "\n",
        "# моделирование обучающей выборки\n",
        "N = 1000\n",
        "x1 = np.random.multivariate_normal(mean1, V, N).T\n",
        "x2 = np.random.multivariate_normal(mean2, V, N).T\n",
        "\n",
        "x_train = np.hstack([x1, x2]).T\n",
        "y_train = np.hstack([np.zeros(N), np.ones(N)])\n",
        "\n",
        "# вычисление оценок МО и общей ковариационной матрицы\n",
        "mm1 = np.mean(x1.T, axis=0)\n",
        "mm2 = np.mean(x2.T, axis=0)\n",
        "\n",
        "a = np.hstack([(x1.T - mm1).T, (x2.T - mm2).T])\n",
        "VV_manual = np.array([\n",
        "    [np.dot(a[0], a[0]) / (N*2-1), np.dot(a[0], a[1]) / (N*2-1), np.dot(a[0], a[2]) / (N*2-1)],\n",
        "    [np.dot(a[1], a[0]) / (N*2-1), np.dot(a[1], a[1]) / (N*2-1), np.dot(a[1], a[2]) / (N*2-1)],\n",
        "    [np.dot(a[2], a[0]) / (N*2-1), np.dot(a[2], a[1]) / (N*2-1), np.dot(a[2], a[2]) / (N*2-1)]\n",
        "])\n",
        "\n",
        "# Проверка\n",
        "data = np.vstack([x1.T - mm1, x2.T - mm2])\n",
        "VV = np.cov(data.T)\n",
        "\n",
        "VV_manual == VV\n",
        "\n",
        "# Проверка равенства\n",
        "is_equal = np.allclose(VV_manual, VV)\n",
        "\n",
        "print(\"Матрица ковариации вручную:\\n\", VV_manual)\n",
        "print(\"Ожидаемая матрица ковариации:\\n\", VV)\n",
        "print(\"Равенство:\", is_equal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNlNP83Wq0FD",
        "outputId": "9d30cd53-1962-4a6c-8af9-ed4f5e08c721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица ковариации вручную:\n",
            " [[1.91375482 1.32262886 0.90497381]\n",
            " [1.32262886 1.9039018  1.31898097]\n",
            " [0.90497381 1.31898097 1.9172694 ]]\n",
            "Ожидаемая матрица ковариации:\n",
            " [[1.91375482 1.32262886 0.90497381]\n",
            " [1.32262886 1.9039018  1.31898097]\n",
            " [0.90497381 1.31898097 1.9172694 ]]\n",
            "Равенство: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Виталий Золотов https://stepik.org/lesson/1370101/step/5?discussion=10354853&thread=solutions&unit=1386291\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Исходные параметры распределений двух классов\n",
        "mean1 = np.array([1, -2, 0])\n",
        "mean2 = np.array([1, 3, 1])\n",
        "r = 0.7\n",
        "D = 2.0\n",
        "V = [[D, D * r, D*r*r], [D*r, D, D*r], [D*r*r, D*r, D]]\n",
        "\n",
        "# Моделирование обучающей выборки\n",
        "N = 1000\n",
        "x1 = np.random.multivariate_normal(mean1, V, N).T\n",
        "x2 = np.random.multivariate_normal(mean2, V, N).T\n",
        "\n",
        "# Объединение выборок\n",
        "x_train = np.hstack([x1, x2]).T\n",
        "y_train = np.hstack([np.zeros(N), np.ones(N)])\n",
        "\n",
        "# Вычисление оценок математического ожидания и ковариационной матрицы\n",
        "mu1 = np.mean(x1.T, axis=0)  # Математическое ожидание для класса 0\n",
        "mu2 = np.mean(x2.T, axis=0)  # Математическое ожидание для класса 1\n",
        "\n",
        "# Объединяем центрированные данные\n",
        "data = np.vstack([x1.T - mu1, x2.T - mu2])\n",
        "\n",
        "# Вычисляем ковариационную матрицу\n",
        "VV = np.cov(data.T)\n",
        "\n",
        "# Обратная ковариационная матрица\n",
        "Sigma_inv = np.linalg.inv(VV)\n",
        "\n",
        "# Параметры для дискриминанта Фишера\n",
        "P_y1 = 0.5  # априорная вероятность класса 0\n",
        "P_y2 = 1 - P_y1  # априорная вероятность класса 1\n",
        "lambda_y1, lambda_y2 = 1, 1  # величины штрафов неверной классификации\n",
        "\n",
        "# Вычисление d1 и d2 для всех обучающих образцов\n",
        "alpha1 = Sigma_inv @ mu1\n",
        "alpha2 = Sigma_inv @ mu2\n",
        "beta1 = np.log(lambda_y1 * P_y1) - 0.5 * mu1 @ Sigma_inv @ mu1\n",
        "beta2 = np.log(lambda_y2 * P_y2) - 0.5 * mu2 @ Sigma_inv @ mu2\n",
        "d1 = x_train @ alpha1 + beta1\n",
        "d2 = x_train @ alpha2 + beta2"
      ],
      "metadata": {
        "id": "ElJmsUV4ogo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}