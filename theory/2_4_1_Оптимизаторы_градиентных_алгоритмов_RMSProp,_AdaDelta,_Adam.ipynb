{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXtawSErtsc4r7RImrImwJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "http://mech.math.msu.su/~vvb/MasterAI/GradientDescent.html"
      ],
      "metadata": {
        "id": "g9vfPc40nFTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent\n",
        "\n",
        "<h2>\n",
        "Нахождение локального минимума функции от нескольких переменных:<br/>\n",
        "метод градиентного спуска и его модификации\n",
        "</h2>\n",
        "\n",
        "<h3>Постановка задачи</h3>\n",
        "\n",
        "<p>\n",
        "Дана непрерывно дифференцируемая функция от нескольких переменных:\n",
        "$$\n",
        "    f: \\Bbb{R}^n \\to \\Bbb{R}\n",
        "$$\n",
        "Нужно найти минимум этой функции &mdash; точнее, точку $x$,\n",
        "в которой функция принимает минимальное значение. Мы будем искать\n",
        "точку локального минимума $x_*$, в которой градиент функции равен нулевому\n",
        "вектору или очень мал:\n",
        "$$\n",
        "        ||\\nabla f(x_*)|| < \\varepsilon.\n",
        "$$\n",
        "</p>\n",
        "\n",
        "<h3>Метод градиентного спуска</h3>\n",
        "\n",
        "<p>\n",
        "В методе градиентного спуска нам дается начальное приближение $x_0$.\n",
        "Предполагается, что в некоторой окрестности минимума функция выпукла вниз\n",
        "и точка $x_0$ принадлежит этой окрестности. Метод градиентного спуска\n",
        "носит итерационный характер: мы вычисляем последовательность приближений\n",
        "$x_0, x_1, x_2, x_3, \\dots$, в которой текущая точка движется к\n",
        "точке минимума. Количество итераций заранее неизвестно, и даже сходимость\n",
        "метода гарантирована далеко не всегда. Хотя теоретически можно указать\n",
        "достаточные условия сходимости, а также оценить ее скорость.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Параметром в методе градиентного спуска является размер шага $\\alpha$,\n",
        "в методах машинного обучения его называют также скоростью обучения &mdash;\n",
        "<i>learning rate</i>. Пусть $x_k$ &mdash; очередное приближение,\n",
        "тогда следующее приближение вычисляется по формуле\n",
        "$$\n",
        "    x_{k+1} = x_k - \\alpha \\cdot \\nabla f(x_k)\n",
        "$$\n",
        "Здесь $\\nabla f(x_k)$ &mdash; вектор градиента функции $f$ в точке $x_k$,\n",
        "указывающий направление роста функции. Вектор противоположного направления\n",
        "$-\\nabla f(x_k)$ иногда называют антиградиентом. Мы смещаемся в направлении\n",
        "убывания функции на величину $\\alpha \\cdot \\nabla f(x_k)$, пропорциональную\n",
        "длине вектора градиента, здесь $\\alpha$ &mdash; коэффициент пропорциональности.\n",
        "Чем больше $\\alpha$, тем быстрее процесс сходится, однако слишком большое\n",
        "значение $\\alpha$ может привести к осцилляции либо вообще к уходу\n",
        "от точки минимума. Примитивное объяснение состоит в том, что при большой\n",
        "величине шага текущая точка проскакивает область убывания\n",
        "функции вблизи очередной точки итерации и входит в зону возрастания,\n",
        "затем возвращается назад, опять проскакивая область убывания и т.д.,\n",
        "причем процесс может даже не сходиться.\n",
        "Такая осцилляция видна на следующих скриншотах программы нахождения\n",
        "минимума: в первом случае коэффициент $\\alpha = 0.1$\n",
        "невелик и программа находит\n",
        "хорошее приближение для точки минимума, а траектория точки получается\n",
        "достаточно гладкой (число итераций не более 100,\n",
        "$\\varepsilon = 10^{-5}$):<br/>\n",
        "\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/GradientDescent2.png\" width=600>\n",
        "\n",
        "<br/>\n",
        "Однако при большей величине шага $\\alpha = 0.4$ начинается осцилляция:<br/>\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/GradientDescent3.png\"><br/>\n",
        "В данном случае процесс все еще сходится, но уже при $\\alpha = 0.5$\n",
        "траектория не попадает в точку минимума, а амплитуда осцилляций стремится\n",
        "к бесконечности:<br/>\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/GradientDescent4.png\"><br/>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Отметим, что осцилляция чаще всего возникает, когда линии уровня\n",
        "функции $f(x)$ в окрестности точки минимума предсталяют собой\n",
        "сильно вытянутые эллипсы (в $n$-мерном случае эллипсоиды с сильно\n",
        "отличающимися длинами полуосей). Это происходит в том случае, когда\n",
        "мера обусловленности матрицы вторых произодных функции (Гессиана)\n",
        "велика, т.е. велико отношение ее максимального собственного значения к\n",
        "минимальному. В двумерном случае поверхность уровня функции представляет\n",
        "собой сильно вытянутую яму, больше напоминающую овраг, и текущая точка\n",
        "перескакивает с одного берега оврага на другой и обратно:<br/>\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/GradientDescent5.png\"><br/>\n",
        "\n",
        "<h3>Достаточные условия сходимости метода градиентного спуска и\n",
        "оценка скорости сходимости</h3>\n",
        "\n",
        "<p>\n",
        "Пусть известно, что в некоторой окрестности точки минимума\n",
        "функция $f(x)$ выпукла вниз и начальное приближение $x_0$\n",
        "принадлежит этой окрестности. Пусть также функция удовлетворяет\n",
        "условию Липшица на ее градиент с константой $L$:\n",
        "$$\n",
        "    ||\\nabla f(x_1) - \\nabla f(x_2)|| \\le L ||x_1 - x_2||\n",
        "$$\n",
        "При размере шага $\\alpha < 2/L$ метод градиентного спуска сходится к точке минимума\n",
        "$x_*$, причем справедлива следующая оценка сверху для числа итераций $N$:\n",
        "$$\n",
        "    N = O(LR^2/\\varepsilon),\n",
        "$$\n",
        "где $R = ||x_0 - x_*||$ &mdash; расстояние от точки начального приближения\n",
        "до точки минимума.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "На практике параметр \"размер шага\" (или learning rate) полагается равным\n",
        "$1/L$, если, конечно, есть возможность оценить $L$.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Однако построены несложные модификации метода градиентного спуска,\n",
        "работающие значительно быстрее, причем вероятность осцилляций в них\n",
        "гораздо ниже. Мы рассмотрим два таких метода: метод\n",
        "тяжелого шарика Поляка и ускоренный метод Нестерова. Последний метод\n",
        "является наилучшим из рассмотренных трех и очень популярен в\n",
        "алгоритмах машинного обучения.\n",
        "</p>\n",
        "\n",
        "<h3>Метод тяжелого шарика Поляка</h3>\n",
        "\n",
        "<p>\n",
        "Англоязычное название этого метода: Heavy Ball.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "В классическом методе градиентного спуска при очередной итерации\n",
        "точка всегда сдвигается в направлении антиградиента. Как видно на\n",
        "приведенных выше иллюстрациях, это может приводить к осцилляциям,\n",
        "когда мера обусловленности гессиана минимизируемой функции большая\n",
        "и шаг градиентного спуска тоже достаточно большой.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Идея метода тяжелого шарика в том, что мы как бы добавляем инерцию\n",
        "к движению точки. Степень инерции регулируется параметром $\\beta$,\n",
        "который обычно в алгоритмах машинного обучения называется моментом\n",
        "(англ. <i>momentum</i>). На очередном шаге алгоритма точка сдвигается\n",
        "на вектор антиградиента, умноженный на длину шага $\\alpha$,\n",
        "к которому дополнительно прибавляется вектор сдвига, вычисленный на\n",
        "предыдущем шаге, умноженный на параметр $\\beta$. Значение параметра\n",
        "$\\beta$ обязательно должно быть меньше единицы (иначе, разогнавшись,\n",
        "мы уже не остановимся); чем больше $\\beta$, тем\n",
        "больше инерция влияет на движение точки, которая скатывается в направлении\n",
        "минимума подобно тяжелому шарику. При этом инерция влияет на уменьшение\n",
        "осцилляций в случае плохо обусловленной матрицы гессиана\n",
        "функции $f$ (точка быстрее выходит на большую полуось эллипсоида).\n",
        "Если параметр $\\beta$ равен нулю,\n",
        "то метод тяжелого шарика превращается в обычный метод градиентного спуска.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Значение $x_{k+1}$ при очередной итерации в методе тяжелого шарика\n",
        "вычисляется по формуле\n",
        "$$\n",
        "    x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta(x_k - x_{k-1}),\\quad\n",
        "    k \\ge 1, \\quad 0 \\le \\beta < 1.\n",
        "$$\n",
        "На следующей иллюстрации показаны трактории движения точки при обычном\n",
        "методе градиентного спуска (красная линия) и в методе тяжелого шарика\n",
        "(синяя линия):<br/>\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/HeavyBall1.png\" /><br/>\n",
        "Здесь размер шага $\\alpha=0.1$ невелик и осцилляций в методе градиентного\n",
        "спуска не происходит. Увеличив $\\alpha = 0.4$, получим такую картинку:<br/>\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/HeavyBall2.png\" /><br/>\n",
        "(Мы также уменьшили параметр $\\beta = 0.2$.)\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "На последней иллюстрации видно, что, помимо уменьшения осцилляций,\n",
        "метод тяжелого шарика сходится быстрее: 25 итераций против 42 в простом\n",
        "методе градиентного спуска.\n",
        "</p>\n",
        "\n",
        "<h3>Ускоренный метод градиентного спуска Нестерова</h3>\n",
        "\n",
        "<p>\n",
        "Ускоренный метод Нестерова значительно улучшает сходимость и скорость\n",
        "градиентного спуска, при том, что его программная реализация\n",
        "ничуть не сложнее. В методе Нестерова используется та же идея, что и в\n",
        "методе тяжелого шарика. Различие с методом тяжелого шарика в том,\n",
        "что при очередной итерации антиградиент функции вычисляется не в текущей\n",
        "точке $x_k$, а в точке $y_k$, в которую сдвинулась бы точка $x_k$\n",
        "с учетом только инерции. Применяются следующие формулы:\n",
        "$$\n",
        "    \\begin{array}{l}\n",
        "    y_0 = x_0                            \\\\\n",
        "    x_1 = y_0 - \\alpha \\nabla f(y_0)     \\\\\n",
        "    y_1 = x_1 + \\beta (x_1 - x_0)        \\\\\n",
        "    \\dots                                \\\\\n",
        "    x_{k+1} = y_k - \\alpha \\nabla f(y_k) \\\\\n",
        "    y_{k+1} = x_{k+1} + \\beta(x_{k+1} - x_k)\n",
        "    \\end{array}\n",
        "$$\n",
        "Неформально говоря, мы как бы прогнозируем будущее движение точки,\n",
        "учитывая ее инерцию, и на очередной итерации используем\n",
        "значение антиградиента функции не в текущей,\n",
        "а в прогнозируемой точке. (Аналогия с движением\n",
        "автомобиля: входя в поворот, мы учитываем его кривизну не в самом начале\n",
        "поворота, а в той его точке, куда мы попадем через некоторый промежуток\n",
        "времени.)\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Следующий рисунок иллюстрирует метод Нестерова:<br/>\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/NesterovAcceleration.png\" />\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Вот скриншот программы, иллюстрирующей\n",
        "ускоренные методы градиентного спуска.\n",
        "Зеленая траектория соответствует методу Нестерова, синяя &mdash;\n",
        "методу тяжелого шарика:<br/>\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/Nesterov1.png\" /><br/>\n",
        "Видно, что в случае метода Нестерова инерция в меньшей степени влияет\n",
        "на отклонение траектории от оптимальной, и при данных параметрах\n",
        "метод Нестерова сходится быстрее.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Сравнение всех трех методов градиентного\n",
        "спуска приведено на следующем скриншоте:\n",
        "<img src=\"http://mech.math.msu.su/~vvb/MasterAI/GradDescentCompare.png\" /><br/>\n",
        "В данном случае метод Нестерова сходится быстрее всех.\n",
        "</p>\n",
        "\n",
        "<h3>Оценка скорости сходимости метода Нестерова</h3>\n",
        "\n",
        "<p>\n",
        "Пусть непрерывно дифференцируемая функция $f(x)$ удовлетворяет\n",
        "условию Липшица для градиента с константой $L$:\n",
        "$$\n",
        "    ||\\nabla f(x_1) - \\nabla f(x_2)|| \\le L ||x_1 - x_2||\n",
        "$$\n",
        "Пусть параметры $\\alpha$ (размер шага, скорость обучения) и\n",
        "$\\beta$ (степень инерции, момент) удовлетворяют неравенствам:\n",
        "$$\n",
        "    0 \\le \\beta < 1, \\quad 0 \\le \\alpha < 2(1 - \\beta)/L.\n",
        "$$\n",
        "Тогда методы Тяжелого шарика и Нестерова сходятся.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "В методе Нестерова обычно выбирается длина шага\n",
        "$$\n",
        "    \\alpha = 1/L.\n",
        "$$\n",
        "Справедлива следующая оценка на число итераций $N$ для получения точности\n",
        "$\\varepsilon$ (т.е. выполнения неравенства\n",
        "$|f(x_N) - f(x_*)| \\le \\varepsilon$):\n",
        "$$\n",
        "    N = O(LR^2/\\sqrt{\\varepsilon}).\n",
        "$$\n",
        "Здесь $R$ &mdash; расстояние от точки начального приближения\n",
        "до точки минимума: $R = ||x_0 - x_*||$.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Сравним со скоростью сходимости классического метода градиентного спуска:\n",
        "$$\n",
        "    N = O(LR^2/\\varepsilon).\n",
        "$$\n",
        "Мы видим, что линейная зависимость числа итераций $N$ от величины\n",
        "$1/\\varepsilon$, обратной требуемой точности, заменилась на зависимость\n",
        "от квадратного корня из $1/\\varepsilon$. То есть при улучшении точности\n",
        "в 100 раз метод градиентного спуска будет работать в 100 раз\n",
        "дольше, а метод Нестерова замедлится только в 10 раз. Причем для\n",
        "сильно выпуклых функций в случае метода Нестерова\n",
        "уже будет справедлива намного более сильная логарифмическая оценка,\n",
        "но мы ее здесь не приводим.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "BtEvj7BmncId"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diL1H5Zzm9fU"
      },
      "outputs": [],
      "source": []
    }
  ]
}
