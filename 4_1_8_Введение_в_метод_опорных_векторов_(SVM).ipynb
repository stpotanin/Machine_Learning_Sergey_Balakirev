{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+KY3hbI5qE2ioWRus9ntD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1370102/step/8"
      ],
      "metadata": {
        "id": "gwK_jvi05RTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yGexMzZ05Jlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bb185f-35f7-415d-cbeb-4baa05eae845"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(6)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# сигмоидная функция потерь\n",
        "def loss(w, x, y):\n",
        "    M = x @ w * y\n",
        "    return (2 / (1 + np.exp(M))).mean()\n",
        "\n",
        "\n",
        "# производная сигмоидной функции потерь по вектору w\n",
        "def df(w, x, y):\n",
        "    M = x @ w * y\n",
        "    return (-2 * np.exp(M) * x.T * y / (1 + np.exp(M)) ** 2).mean(axis=1)\n",
        "\n",
        "# Данные\n",
        "data_x = [(5.8, 1.2), (5.6, 1.5), (6.5, 1.5), (6.1, 1.3), (6.4, 1.3), (7.7, 2.0), (6.0, 1.8), (5.6, 1.3), (6.0, 1.6), (5.8, 1.9), (5.7, 2.0), (6.3, 1.5), (6.2, 1.8), (7.7, 2.3), (5.8, 1.2), (6.3, 1.8), (6.0, 1.0), (6.2, 1.3), (5.7, 1.3), (6.3, 1.9), (6.7, 2.5), (5.5, 1.2), (4.9, 1.0), (6.1, 1.4), (6.0, 1.6), (7.2, 2.5), (7.3, 1.8), (6.6, 1.4), (5.6, 2.0), (5.5, 1.0), (6.4, 2.2), (5.6, 1.3), (6.6, 1.3), (6.9, 2.1), (6.8, 2.1), (5.7, 1.3), (7.0, 1.4), (6.1, 1.4), (6.1, 1.8), (6.7, 1.7), (6.0, 1.5), (6.5, 1.8), (6.4, 1.5), (6.9, 1.5), (5.6, 1.3), (6.7, 1.4), (5.8, 1.9), (6.3, 1.3), (6.7, 2.1), (6.2, 2.3), (6.3, 2.4), (6.7, 1.8), (6.4, 2.3), (6.2, 1.5), (6.1, 1.4), (7.1, 2.1), (5.7, 1.0), (6.8, 1.4), (6.8, 2.3), (5.1, 1.1), (4.9, 1.7), (5.9, 1.8), (7.4, 1.9), (6.5, 2.0), (6.7, 1.5), (6.5, 2.0), (5.8, 1.0), (6.4, 2.1), (7.6, 2.1), (5.8, 2.4), (7.7, 2.2), (6.3, 1.5), (5.0, 1.0), (6.3, 1.6), (7.7, 2.3), (6.4, 1.9), (6.5, 2.2), (5.7, 1.2), (6.9, 2.3), (5.7, 1.3), (6.1, 1.2), (5.4, 1.5), (5.2, 1.4), (6.7, 2.3), (7.9, 2.0), (5.6, 1.1), (7.2, 1.8), (5.5, 1.3), (7.2, 1.6), (6.3, 2.5), (6.3, 1.8), (6.7, 2.4), (5.0, 1.0), (6.4, 1.8), (6.9, 2.3), (5.5, 1.3), (5.5, 1.1), (5.9, 1.5), (6.0, 1.5), (5.9, 1.8)]\n",
        "data_y = [-1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1]\n",
        "x_train = np.array([[1] + list(x) for x in data_x])\n",
        "y_train = np.array(data_y)\n",
        "\n",
        "# Параметры\n",
        "n_train = len(x_train)  # размер обучающей выборки\n",
        "w = [0.0, 0.0, 0.0]  # начальные весовые коэффициенты\n",
        "nt = np.array([1, 0.1, 0.1])  # шаг обучения для каждого параметра w0, w1, w2\n",
        "lm = 0.01  # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
        "N = 500  # число итераций алгоритма SGD\n",
        "batch_size = 10 # размер мини-батча (величина K = 10)\n",
        "\n",
        "Qe = (x_train @ w * y_train < 0).sum()\n",
        "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
        "for _ in range(N):\n",
        "    k = np.random.randint(0, n_train-batch_size-1) # n_train - размер выборки (массива x_train)\n",
        "    b = range(k, k+batch_size)\n",
        "    Qk = loss(w, x_train[b], y_train[b])\n",
        "    Qe = lm * Qk + (1 - lm) * Qe\n",
        "    grad = df(w, x_train[b], y_train[b])\n",
        "    w -= nt * grad\n",
        "\n",
        "Q = (x_train @ w * y_train < 0).sum()\n",
        "\n",
        "Q"
      ]
    }
  ]
}