{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkGk+LnRSZhPM34I9m9ieJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1370096/step/8"
      ],
      "metadata": {
        "id": "Gq4mbOuQZrkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPENN1m2Zn6y",
        "outputId": "c4f3ee8d-73bb-42f5-a3b9-308cdbff00b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "w = np.array([20, -1, 3, 5, -0.15])\n",
        "p = .5\n",
        "M = -np.log(1/p - 1)\n",
        "\n",
        "x = [M / (x * len(w)) for x in w]\n",
        "x @ w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "w = np.array([20, -1, 3, 5, -0.15])\n",
        "p = .5\n",
        "M = -np.log(1/p - 1)\n",
        "\n",
        "x = np.zeros(len(w))\n",
        "x[0] = M / w[0]\n",
        "\n",
        "x @ w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KwG88UxzAyx",
        "outputId": "9270c809-3d7a-4638-d7ba-519ab4e6b639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В логистической регрессии $$ w_0 $$ играет роль свободного члена (интерсепта), но его интерпретация может отличаться от традиционного понимания в линейной регрессии.\n",
        "\n",
        "## Логистическая регрессия и её структура\n",
        "\n",
        "Логистическая регрессия используется для предсказания вероятности бинарного события. Модель описывается следующим уравнением:\n",
        "\n",
        "$$\n",
        "P(y=1|x) = g(w^T x + w_0) = \\frac{1}{1 + e^{-(w^T x + w_0)}}\n",
        "$$\n",
        "\n",
        "где $$ g(z) $$ — логистическая функция, $$ w $$ — вектор коэффициентов, а $$ x $$ — вектор признаков. В этом контексте $$ w_0 $$ добавляется как свободный член, чтобы учесть влияние, не зависящее от значений признаков $$ x $$[3][4].\n",
        "\n",
        "## Интерпретация $$ w_0 $$\n",
        "\n",
        "В отличие от линейной регрессии, где интерсепт можно интерпретировать как значение зависимой переменной при нулевых значениях всех независимых переменных, в логистической регрессии $$ w_0 $$ определяет смещение логистической функции. Это значение влияет на вероятность события, но не всегда имеет прямую интерпретацию в контексте предсказываемой зависимости[1][2].\n",
        "\n",
        "### Пример\n",
        "\n",
        "Если $$ w_0 > 0 $$, это означает, что даже при нулевых значениях всех признаков вероятность события будет выше 0.5, что может быть важно для понимания базового уровня риска или вероятности события. Если $$ w_0 < 0 $$, то вероятность события будет ниже 0.5 при нулевых значениях признаков[4][5].\n",
        "\n",
        "Таким образом, хотя $$ w_0 $$ и является интерсептом, его роль в логистической регрессии более сложна и связана с вероятностной природой модели.\n",
        "\n",
        "Citations:\n",
        "[1] https://www.bsuir.by/m/12_100229_1_191227.pdf\n",
        "[2] https://your-scorpion.ru/linear-regression-python/\n",
        "[3] https://ru.wikipedia.org/wiki/%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F\n",
        "[4] https://blog.skillfactory.ru/glossary/logisticheskaya-regressiya/\n",
        "[5] http://www.machinelearning.ru/wiki/images/archive/c/c6/20150420141547!Voron-ML-Bayes2-slides.pdf\n",
        "[6] https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/homeworks-practice/homework-practice-02.ipynb?short_path=2682fa6"
      ],
      "metadata": {
        "id": "1acwq-IO3Mqt"
      }
    }
  ]
}