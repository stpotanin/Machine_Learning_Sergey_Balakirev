{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnfluUo+Ux1k9g4mi2tMZG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1370105/step/7"
      ],
      "metadata": {
        "id": "WbOVvLCBsYFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbxfV178sT2K",
        "outputId": "95e21e3b-ecfd-4fb5-ba09-84de3cb8ecd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.955"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Функции\n",
        "# Экспоненциальная функция потерь e^{-w^T⋅x_i​⋅y_i​} для одного i-го наблюдения\n",
        "# loss = lambda x, y, w: np.exp(x @ w * y)\n",
        "# Её производная по вектору параметров w = -e^{-w^T⋅x_i​⋅y_i​}⋅x_i^T​⋅y_i\n",
        "d_loss = lambda x, y, w: -np.exp(-x @ w * y) * x.T * y\n",
        "# Отступ\n",
        "margin = lambda x_test, y_test: [x @ w * y for x, y in zip(x_test, y_test)]\n",
        "# Accuracy TODO\n",
        "accuracy = lambda x_test, y_test: (np.sign(margin(x_test, y_test)) == 1).mean()\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# исходные параметры распределений двух классов\n",
        "r1 = 0.4\n",
        "D1 = 2.0\n",
        "mean1 = [1, -2]\n",
        "V1 = [[D1, D1 * r1], [D1 * r1, D1]]\n",
        "\n",
        "r2 = 0.5\n",
        "D2 = 3.0\n",
        "mean2 = [2, 3]\n",
        "V2 = [[D2, D2 * r2], [D2 * r2, D2]]\n",
        "\n",
        "# моделирование обучающей выборки\n",
        "N = 1000\n",
        "x1 = np.random.multivariate_normal(mean1, V1, N).T\n",
        "x2 = np.random.multivariate_normal(mean2, V2, N).T\n",
        "\n",
        "data_x = np.array([[1, x[0], x[1]] for x in np.hstack([x1, x2]).T]) # столбец из 1\n",
        "data_y = np.hstack([np.ones(N) * -1, np.ones(N)])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, random_state=123,test_size=0.3, shuffle=True)\n",
        "\n",
        "n_train = len(x_train)  # размер обучающей выборки\n",
        "w = [0.0, 0.0, 0.0]     # начальные весовые коэффициенты\n",
        "nt = np.array([0.5, 0.01, 0.01])  # шаг обучения для весов вектора w\n",
        "lm = 0.01               # значение параметра лямбда для вычисления EMA\n",
        "N = 500                 # число итераций алгоритма SGD\n",
        "batch_size = 10         # размер мини-батча (величина K = 10)\n",
        "\n",
        "\n",
        "for _ in range(N):\n",
        "    k = np.random.randint(0, n_train-batch_size-1)\n",
        "    b = range(k, k+batch_size)\n",
        "    grad = np.mean([d_loss(x, y, w) for x, y in zip(x_train[b], y_train[b])], axis=0)\n",
        "    w -= nt * grad\n",
        "\n",
        "mrgs = sorted(margin(x_test, y_test))\n",
        "acc = accuracy(x_test, y_test)\n"
      ]
    }
  ]
}
