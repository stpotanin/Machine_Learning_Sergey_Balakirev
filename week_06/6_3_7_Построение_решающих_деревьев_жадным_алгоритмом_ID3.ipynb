{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg67V/8HTYd7409BaYE2Ag"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1370121/step/7"
      ],
      "metadata": {
        "id": "Ld8lLhbJ2vVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQIiusOc2q78",
        "outputId": "ede4b6b2-e654-4ab8-eedf-d5178100e855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Номер признака 0, Оптимальный порог 5.500, Информационный выигрыш 0.304322\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "data_x = [(5.8, 2.7), (6.7, 3.1), (5.7, 2.9), (5.5, 2.4), (4.8, 3.4), (5.4, 3.4), (4.8, 3.0), (5.5, 2.5), (5.3, 3.7), (7.0, 3.2), (5.6, 2.9), (4.9, 3.1), (4.8, 3.0), (5.0, 2.3), (5.2, 3.4), (5.1, 3.8), (5.0, 3.0), (5.0, 3.3), (4.6, 3.1), (5.5, 2.6), (5.0, 3.5), (6.7, 3.0), (6.0, 2.2), (4.8, 3.1), (6.4, 2.9), (5.6, 3.0), (4.4, 3.0), (4.9, 2.4), (5.6, 3.0), (5.0, 3.6), (5.1, 3.3), (5.8, 4.0), (5.5, 2.4), (5.2, 2.7), (5.1, 3.8), (5.1, 3.5), (5.5, 4.2), (4.9, 3.1), (5.9, 3.2), (5.7, 2.6), (4.7, 3.2), (5.4, 3.9), (5.8, 2.6), (5.1, 3.4), (6.4, 3.2), (5.8, 2.7), (5.6, 2.7), (5.7, 2.8), (5.4, 3.0), (5.0, 3.2), (4.6, 3.4), (6.0, 2.7), (6.6, 3.0), (4.9, 3.0), (4.9, 3.6), (4.4, 3.2), (5.4, 3.4), (6.0, 3.4), (5.9, 3.0), (6.1, 2.8), (5.1, 3.7), (5.5, 3.5), (6.1, 3.0), (6.2, 2.2), (5.7, 3.0), (5.2, 3.5), (5.4, 3.7), (4.6, 3.2), (5.2, 4.1), (5.0, 2.0), (6.8, 2.8), (5.0, 3.5), (6.7, 3.1), (6.3, 3.3), (6.0, 2.9), (4.7, 3.2), (6.6, 2.9), (5.6, 2.5), (4.4, 2.9), (6.2, 2.9), (6.1, 2.9), (4.3, 3.0), (6.9, 3.1), (5.7, 3.8), (5.4, 3.9), (6.1, 2.8), (4.6, 3.6), (5.5, 2.3), (4.8, 3.4), (6.5, 2.8), (6.3, 2.5), (5.1, 3.8), (5.7, 4.4), (5.0, 3.4), (4.5, 2.3), (5.7, 2.8), (5.1, 2.5), (5.1, 3.5), (6.3, 2.3), (5.0, 3.4)]\n",
        "data_y = [1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1]\n",
        "\n",
        "x_train = np.array(data_x)\n",
        "y_train = np.array(data_y)\n",
        "\n",
        "# Вероятность\n",
        "p = lambda i, X: sum([x == i for x in X]) / len(X)\n",
        "\n",
        "# Энтропия (Шеннон)\n",
        "# S = lambda X: -sum([p(i, X) * np.log2(p(i, X)) for i in (0, 1) if p(i, X) != 0])\n",
        "\n",
        "# Энтропия (Джини)\n",
        "S = lambda X: 1 - sum([p(i, X)**2 for i in set(y_train)])\n",
        "\n",
        "# Информационный выигрыш (Information Gain)\n",
        "inf_gain = lambda X_new, X: S_0 - sum([len(X_new[i]) / len(X) * S(X_new[i]) for i in range(len(X_new))])\n",
        "\n",
        "# Начальные установки перед поиском\n",
        "S_0 = S(y_train)\n",
        "IG = 0\n",
        "\n",
        "# Поиск перебором\n",
        "for j in (0, 1):\n",
        "    range_t = np.arange(min(x_train[:, j])+0.1, max(x_train[:, j])-0.1, 0.1)\n",
        "    for t in range_t:\n",
        "        y_new = [[y for x, y in zip(x_train, y_train) if x[j] <= t],\n",
        "                [y for x, y in zip(x_train, y_train) if x[j] > t]]\n",
        "        ig = inf_gain(y_new, y_train)\n",
        "        if ig > IG:\n",
        "            fj = j\n",
        "            IG = ig\n",
        "            th = t\n",
        "\n",
        "# print(f\"Номер признака {fj}, Оптимальный порог {th:.3f}, Информационный выигрыш {IG:.6f}\")"
      ]
    }
  ]
}
