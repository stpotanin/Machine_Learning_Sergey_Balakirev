{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmH6W9JX+0b/DT8BNyaj5X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1370120/step/8"
      ],
      "metadata": {
        "id": "wAv2gGGKXzpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVXWSMTnXwwp",
        "outputId": "796bac05-4aa7-40a0-b87f-a2bf9da11bcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49995"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 2, size=200)\n",
        "# print(X)\n",
        "\n",
        "# Вероятность\n",
        "p = lambda i, X: sum([x == i for x in X]) / len(X)\n",
        "\n",
        "# Энтропия (Шеннон)\n",
        "# S = lambda X: -sum([p(i, X) * np.log2(p(i, X)) for i in (0, 1) if p(i, X) != 0])\n",
        "\n",
        "# Энтропия (Джини)\n",
        "S = lambda X: 1 - sum([p(i, X)**2 for i in (0, 1)])\n",
        "\n",
        "# Информационный выигрыш (Information Gain)\n",
        "inf_gain = lambda X_new, X: S_0 - sum([len(X_new[i]) / len(X) * S(X_new[i]) for i in range(len(X_new))])\n",
        "\n",
        "S_0 = S(X)\n",
        "X_new = [X[:150], X[150:]]\n",
        "IG = inf_gain(X_new, X)\n",
        "print(X_new)\n",
        "IG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь всё становится яснее. В контексте информационного выигрыша (Information Gain) в теории принятия решений и машинном обучении, особенно при построении деревьев решений, формула имеет следующий смысл:\n",
        "\n",
        "- **Информационный выигрыш** ($ IG $) — это мера того, насколько разбиение набора данных по какому-либо признаку уменьшает неопределенность или энтропию исходных данных.\n",
        "\n",
        "Формула для информационного выигрыша обычно выглядит так:\n",
        "$ IG = H(D) - \\sum_{i=1}^{q} \\frac{|D_i|}{|D|} H(D_i) $\n",
        "\n",
        "Здесь:\n",
        "- $ H(D) $ — энтропия исходного набора данных $ D $.\n",
        "- $ q $ — количество подмножеств (групп), полученных после разбиения.\n",
        "- $ |D_i| / |D| $ — доля элементов в каждой группе.\n",
        "- $ H(D_i) $ — энтропия каждой группы.\n",
        "\n",
        "В нашей формуле:\n",
        "- **$ S_0 = H(D) $**: Это начальная энтропия всего набора данных до его разбиения.\n",
        "- **$ N[i] / NN = |D_i| / |D| $**: Это доля элементов в каждой группе после разбиения.\n",
        "- **$ S[i] = H(D_i) $**: Это энтропия каждой группы.\n",
        "\n",
        "Таким образом, формула информационного выигрыша показывает, насколько применение критерия разбиения уменьшает общую неопределенность или сложность исходных данных."
      ],
      "metadata": {
        "id": "sfQY-qeQaTn_"
      }
    }
  ]
}
