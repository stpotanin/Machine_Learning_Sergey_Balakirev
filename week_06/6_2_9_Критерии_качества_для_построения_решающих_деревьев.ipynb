{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6sAOn000hFXK4sWH6AhnV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1370120/step/9"
      ],
      "metadata": {
        "id": "wAv2gGGKXzpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVXWSMTnXwwp",
        "outputId": "37806396-ac4f-42dd-939e-4fe2d3af99e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Оптимальный порог 5.450, Информационный выигрыш 0.304322\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "data_x = [(5.8, 2.7), (6.7, 3.1), (5.7, 2.9), (5.5, 2.4), (4.8, 3.4), (5.4, 3.4), (4.8, 3.0), (5.5, 2.5), (5.3, 3.7), (7.0, 3.2), (5.6, 2.9), (4.9, 3.1), (4.8, 3.0), (5.0, 2.3), (5.2, 3.4), (5.1, 3.8), (5.0, 3.0), (5.0, 3.3), (4.6, 3.1), (5.5, 2.6), (5.0, 3.5), (6.7, 3.0), (6.0, 2.2), (4.8, 3.1), (6.4, 2.9), (5.6, 3.0), (4.4, 3.0), (4.9, 2.4), (5.6, 3.0), (5.0, 3.6), (5.1, 3.3), (5.8, 4.0), (5.5, 2.4), (5.2, 2.7), (5.1, 3.8), (5.1, 3.5), (5.5, 4.2), (4.9, 3.1), (5.9, 3.2), (5.7, 2.6), (4.7, 3.2), (5.4, 3.9), (5.8, 2.6), (5.1, 3.4), (6.4, 3.2), (5.8, 2.7), (5.6, 2.7), (5.7, 2.8), (5.4, 3.0), (5.0, 3.2), (4.6, 3.4), (6.0, 2.7), (6.6, 3.0), (4.9, 3.0), (4.9, 3.6), (4.4, 3.2), (5.4, 3.4), (6.0, 3.4), (5.9, 3.0), (6.1, 2.8), (5.1, 3.7), (5.5, 3.5), (6.1, 3.0), (6.2, 2.2), (5.7, 3.0), (5.2, 3.5), (5.4, 3.7), (4.6, 3.2), (5.2, 4.1), (5.0, 2.0), (6.8, 2.8), (5.0, 3.5), (6.7, 3.1), (6.3, 3.3), (6.0, 2.9), (4.7, 3.2), (6.6, 2.9), (5.6, 2.5), (4.4, 2.9), (6.2, 2.9), (6.1, 2.9), (4.3, 3.0), (6.9, 3.1), (5.7, 3.8), (5.4, 3.9), (6.1, 2.8), (4.6, 3.6), (5.5, 2.3), (4.8, 3.4), (6.5, 2.8), (6.3, 2.5), (5.1, 3.8), (5.7, 4.4), (5.0, 3.4), (4.5, 2.3), (5.7, 2.8), (5.1, 2.5), (5.1, 3.5), (6.3, 2.3), (5.0, 3.4)]\n",
        "data_y = [1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1]\n",
        "\n",
        "x_train = np.array(data_x)\n",
        "y_train = np.array(data_y)\n",
        "\n",
        "# Вероятность\n",
        "p = lambda i, X: sum([x == i for x in X]) / len(X)\n",
        "\n",
        "# Энтропия (Шеннон)\n",
        "# S = lambda X: -sum([p(i, X) * np.log2(p(i, X)) for i in (0, 1) if p(i, X) != 0])\n",
        "\n",
        "# Энтропия (Джини)\n",
        "S = lambda X: 1 - sum([p(i, X)**2 for i in set(y_train)])\n",
        "\n",
        "# Информационный выигрыш (Information Gain)\n",
        "inf_gain = lambda X_new, X: S_0 - sum([len(X_new[i]) / len(X) * S(X_new[i]) for i in range(len(X_new))])\n",
        "\n",
        "# Начальные установки перед поиском\n",
        "S_0 = S(y_train)\n",
        "t_min, t_max = min([x[0] for x in x_train]), max([x[0] for x in x_train])\n",
        "t0 = t_min\n",
        "IG = 0\n",
        "\n",
        "# Поиск перебором\n",
        "for t in np.arange(t_min, t_max, .05):\n",
        "    y_new = [[y for x, y in zip(x_train, y_train) if x[0] <= t],\n",
        "             [y for x, y in zip(x_train, y_train) if x[0] > t]]\n",
        "    ig = inf_gain(y_new, y_train)\n",
        "    if ig > IG:\n",
        "        IG = ig\n",
        "        t0 = t\n",
        "\n",
        "print(f\"Оптимальный порог {t0:.3f}, Информационный выигрыш {IG:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Информационный выигрыш** ($ IG $) — это мера того, насколько разбиение набора данных по какому-либо признаку уменьшает неопределенность или энтропию исходных данных.\n",
        "\n",
        "Формула для информационного выигрыша обычно выглядит так:\n",
        "$ IG = H(D) - \\sum_{i=1}^{q} \\frac{|D_i|}{|D|} H(D_i) $\n",
        "\n",
        "Здесь:\n",
        "- $ H(D) $ — энтропия исходного набора данных $ D $.\n",
        "- $ q $ — количество подмножеств (групп), полученных после разбиения.\n",
        "- $ |D_i| / |D| $ — доля элементов в каждой группе.\n",
        "- $ H(D_i) $ — энтропия каждой группы.\n",
        "\n",
        "В формуле задания:\n",
        "- **$ S_0 = H(D) $**: Это начальная энтропия всего набора данных до его разбиения.\n",
        "- **$ N[i] / NN = |D_i| / |D| $**: Это доля элементов в каждой группе после разбиения.\n",
        "- **$ S[i] = H(D_i) $**: Это энтропия каждой группы."
      ],
      "metadata": {
        "id": "sfQY-qeQaTn_"
      }
    }
  ]
}
